import csv
import time
import os
import concurrent.futures
from bs4 import BeautifulSoup
import random
from dotenv import load_dotenv
from curl_cffi import requests
from curl_cffi.requests.errors import RequestsError
from config.get_mongo_connection import get_database, close_connection
from src.get_data_from_env import get_filename
from src.checkpoint_manager import save_checkpoint, load_processed_ids

SOURCE_COLLECTION = 'raw_data'  # Collection ch·ª©a d·ªØ li·ªáu g·ªëc
TARGET_COLLECTION = 'product_names'  # Collection m·ªõi ƒë·ªÉ ch·ª©a k·∫øt qu·∫£
BATCH_SIZE = 100
WORKERS = 15
MAX_RETRY_ROUNDS = 3

GROUP1 = [
    'view_product_detail',
    'select_product_option',
    'select_product_option_quality',
    'add_to_cart_action',
    'product_detail_recommendation_visible',
    'product_detail_recommendation_noticed'
]

GROUP2 = [
    'product_view_all_recommend_clicked'
]

BROWSER_LIST = [
    "chrome124",
    "edge101",
    "safari17_0"
]

# L·∫•y th∆∞ m·ª•c file hi·ªán t·∫°i
current_dir = os.path.dirname(__file__)

# L·∫•y th∆∞ m·ª•c g·ªëc c·ªßa project
project_dir = os.path.abspath(os.path.join(current_dir, '..'))

# L·∫•y th∆∞ m·ª•c file .env
env_path = os.path.join(project_dir, '.env')

load_dotenv(dotenv_path=env_path)

product_name_path = os.environ.get('PRODUCT_NAME_PATH')

OUTPUT_CSV = get_filename(product_name_path, 'PRODUCT_NAME_PATH')

def get_total():
    # H√†m l·∫•y t·ªïng c·ªông b·∫£n ghi th·ªèa m√£n ƒëi·ªÅu ki·ªán c·∫ßn crawl
    db = get_database()
    src_collection = db[SOURCE_COLLECTION]

    # Ch·ªâ l·∫•y nh·ªØng field th·ª±c s·ª± c·∫ßn thi·∫øt
    # _id: 0 nghƒ©a l√† kh√¥ng l·∫•y _id
    field_group = {
        "collection": 1,
        "product_id": 1,
        "viewing_product_id": 1,
        "_id": 0
    }

    # L·ªçc ngay t·ª´ ƒë·∫ßu
    condition_group = {"collection": {"$in": GROUP1 + GROUP2}}

    # L·∫•y con tr·ªè (cursor) v·ªÅ, ch∆∞a t·∫£i data ngay
    cursor = src_collection.find(condition_group, field_group)

    products_set = set()

    for doc in cursor:
        productid = None
        col_type = doc.get("collection")

        if col_type in GROUP1:
            # ∆Øu ti√™n product_id, n·∫øu kh√¥ng c√≥ th√¨ l·∫•y viewing_product_id
            productid = doc.get("product_id") or doc.get("viewing_product_id")
        else:
            # L·∫•y viewing_product_id
            productid = doc.get("viewing_product_id")

        if productid:
            products_set.add(productid)

    return len(products_set)

def get_product(existing_products_set):
    # H√†m l·∫•y c√°c d·ªØ li·ªáu product th·ªèa m√£n ƒëi·ªÅu ki·ªán
    db = get_database()
    src_collection = db[SOURCE_COLLECTION]
    products_set = set()

    condition_group1 = {"collection": {"$in": GROUP1}}
    field_group1 = {"product_id": 1, "viewing_product_id": 1, "current_url": 1}
    doc_group1 = src_collection.find(condition_group1, field_group1)

    yielded_count = 0

    for doc in doc_group1:
        if yielded_count >= 1000:
            print(f"üõë ƒê√£ l·∫•y ƒë·ªß 1000 b·∫£n ghi ƒë·ªÉ test. D·ª´ng generator.")
            break
        product_id = doc.get('product_id') or doc.get('viewing_product_id')
        url = doc.get('current_url')

        # L·ªåC CHECKPOINT: B·ªè qua n·∫øu ƒë√£ c√≥ trong file success productid
        if product_id and product_id in existing_products_set:
            continue

        if product_id and url and isinstance(url, str) and product_id not in products_set:
            products_set.add(product_id)
            # N·∫øu qua ƒë∆∞·ª£c h·∫øt c√°c c·ª≠a ·∫£i th√¨ m·ªõi yield v√† tƒÉng bi·∫øn ƒë·∫øm
            yielded_count += 1
            yield {'product_id': product_id, 'url': url}

    condition_group2 = {"collection": {"$in": GROUP2}}
    field_group2 = {"viewing_product_id": 1, "referrer_url": 1}
    doc_group2 = src_collection.find(condition_group2, field_group2)

    for doc in doc_group2:
        if yielded_count >= 1000:
            print(f"üõë ƒê√£ l·∫•y ƒë·ªß 1000 b·∫£n ghi ƒë·ªÉ test. D·ª´ng generator.")
            break
        product_id = doc.get('viewing_product_id')
        url = doc.get('referrer_url')

        # L·ªåC CHECKPOINT: B·ªè qua n·∫øu ƒë√£ c√≥ trong file success productid
        if product_id and product_id in existing_products_set:
            continue

        if product_id and url and isinstance(url, str) and product_id not in products_set:
            products_set.add(product_id)
            # N·∫øu qua ƒë∆∞·ª£c h·∫øt c√°c c·ª≠a ·∫£i th√¨ m·ªõi yield v√† tƒÉng bi·∫øn ƒë·∫øm
            yielded_count += 1
            yield {'product_id': product_id, 'url': url}

def name_scrapping(item):
    # H√†m crawl d·ªØ li·ªáu t·ª´ url v√† x·ª≠ l√Ω theo t·ª´ng lo·∫°i

    # Gi√∫p request kh√¥ng b·ªã g·ª≠i d·ªìn d·∫≠p c√πng 1 l√∫c -> Server ƒë·ª° nghi ng·ªù
    # time.sleep(random.uniform(0.5, 1))

    product_id = item['product_id']
    url = item['url']

    session = requests.Session()
    # Ch·ªçn ng·∫´u nhi√™n 1 ki·ªÉu tr√¨nh duy·ªát
    browser_type = random.choice(BROWSER_LIST)

    # C·∫•u h√¨nh s·ªë l·∫ßn th·ª≠ l·∫°i
    max_retry = 3

    for attempt in range(max_retry):
        try:
            res = session.get(
                url,
                timeout=10,
                impersonate=browser_type,
                verify=False
            )
            # TR∆Ø·ªúNG H·ª¢P 1: TH√ÄNH C√îNG (200)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, "lxml")
                product_name = "Unknown name"

                # (Logic l·∫•y t√™n gi·ªØ nguy√™n...)
                h1_tag = soup.select_one('h1.page-title span')
                if h1_tag:
                    text = h1_tag.get_text(strip=True)
                    if text: product_name = text

                if product_name == "Unknown name":
                    meta_tag = soup.find('meta', property='og:title')
                    if meta_tag:
                        content = meta_tag.get('content')
                        if content: product_name = content

                return {'product_id': product_id, 'product_name': product_name, 'url': url,
                        'status': 'success', 'http_code': 200, 'error_msg': ''}

            # TR∆Ø·ªúNG H·ª¢P 2: L·ªñI 404 -> KH√îNG RETRY
            elif res.status_code == 404:
                return {'product_id': product_id, 'url': url, 'status': 'err_404',
                        'http_code': 404, 'error_msg': 'Page Not Found'}

            # TR∆Ø·ªúNG H·ª¢P 3: L·ªñI 403 -> RETRY v√† ƒë·ªïi sang browser kh√°c ƒë·ªÉ th·ª≠ l·∫°i (tr√°nh b·ªã web ch·∫∑n)
            elif res.status_code == 403:
                if attempt < max_retry - 1:
                    new_browser = random.choice([b for b in BROWSER_LIST if b != browser_type])
                    browser_type = new_browser
                    print(f"G·∫∑p 403 (v·ªõi productid {product_id}). ƒê·ªïi sang {new_browser} v√† th·ª≠ l·∫°i ngay...")

                    # Ch·ªâ sleep nh·∫π
                    time.sleep(30)
                    continue

                return {'product_id': product_id, 'url': url, 'status': 'err_403',
                        'http_code': 403, 'error_msg': 'Forbidden'}

            # TR∆Ø·ªúNG H·ª¢P 4: L·ªñI 429 -> SLEEP + RETRY
            elif res.status_code == 429:
                print('err_429')
                print(f"429 Rate Limit - ID {product_id}. Sleeping 5s... (Attempt {attempt + 1})")
                if attempt < max_retry - 1:
                    time.sleep(random.uniform(30, 60))
                    continue  # Quay l·∫°i ƒë·∫ßu v√≤ng l·∫∑p ƒë·ªÉ th·ª≠ l·∫°i

                return {'product_id': product_id, 'url': url, 'status': 'err_429',
                        'http_code': 429, 'error_msg': 'Rate limit'}

            # TR∆Ø·ªúNG H·ª¢P 5: L·ªñI 5xx -> RETRY
            elif res.status_code in range (500,600):
                # ƒê√¢y ch√≠nh l√† thay th·∫ø cho status_forcelist c·ªßa HTTPAdapter
                print(f"L·ªói Server {res.status_code} - Retry...")
                if attempt < max_retry - 1:
                    time.sleep(2)
                    continue

                return {'product_id': product_id, 'url': url, 'status': 'err_5xx',
                        'http_code': res.status_code, 'error_msg': f'HTTP {res.status_code}'}

        except RequestsError as e:
            # L·ªñI M·∫†NG KHI REQUEST
            # print(f"L·ªói k·∫øt n·ªëi (M·∫•t m·∫°ng/DNS): {e}")
            if attempt < max_retry - 1:
                time.sleep(2)
                continue
            return {'product_id': product_id, 'url': url, 'status': 'err_network', 'error_msg': str(e)}

        except Exception as e:
            # L·ªñI KH√ÅC
            if attempt < max_retry - 1:
                time.sleep(2)
                continue

            return {'product_id': product_id, 'url': url, 'status': 'err_other',
                    'http_code': 0, 'error_msg': str(e)}

    # N·∫øu h·∫øt v√≤ng l·∫∑p m√† v·∫´n d√≠nh l·ªói kh√°c
    return {'product_id': product_id, 'url': url, 'status': 'err_other',
            'http_code': res.status_code, 'error_msg': f'HTTP {res.status_code}'}


# DATA QUALITY (CHU·∫®N H√ìA & VALIDATE)
def normalize_data(raw_item):
    # H√†m chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u
    return {
        'product_id': str(raw_item.get('product_id', '')).strip(),
        'product_name': str(raw_item.get('product_name', '')).strip(),
        'url': str(raw_item.get('url', '')).strip()
    }

def validate_data(item):
    # H√†m ki·ªÉm tra d·ªØ li·ªáu s·∫°ch
    if not item['product_id']:
        return False, 'missing_id'
    # T√™n s·∫£n ph·∫©m ph·∫£i c√≥ v√† kh√¥ng ƒë∆∞·ª£c l√† "Unknown name"
    if not item['product_name'] or item['product_name'].lower() == 'unknown name':
        return False, 'missing_name'
    if not item['url']:
        return False, 'missing_url'
    return True, 'valid'

def process_results(results_list, csv_writer, stats):
    # H√†m x·ª≠ l√Ω danh s√°ch k·∫øt qu·∫£ (D√πng chung cho c·∫£ batch v√† ph·∫ßn d∆∞)

    insert_mongo_batch = []
    for result in results_list:
        # 1. Ghi log CSV (Ghi t·∫•t c·∫£ k·∫øt qu·∫£, k·ªÉ c·∫£ l·ªói)
        csv_writer.writerow(result)

        # 2. C·∫≠p nh·∫≠t th·ªëng k√™ m·∫°ng
        stats['processed'] += 1
        status_key = result['status']

        if status_key == 'success':
            stats['success'] += 1
        elif status_key in stats:
            stats[status_key] += 1
        else:
            stats['err_other'] += 1

        # 3. X·ª≠ l√Ω CHECKPOINT: Ghi ngay l·∫≠p t·ª©c productid n√†y v√†o file txt t∆∞∆°ng ·ª©ng
        save_checkpoint(result['product_id'], result['status'])

        # 4. X·ª≠ l√Ω Data Quality & chu·∫©n b·ªã ghi v√†o MongoDB
        if status_key == 'success':
            # Chu·∫©n h√≥a
            normalized_item = normalize_data(result)
            # Ki·ªÉm tra
            is_valid, reason = validate_data(normalized_item)

            if is_valid:
                stats['dq_valid'] += 1
                insert_mongo_batch.append(normalized_item)
            else:
                dq_key = f"dq_{reason}"
                if dq_key in stats:
                    stats[dq_key] += 1
    return insert_mongo_batch

def run_crawler_round(round_number, stats, existing_products_set, start_time):
    # H√†m x·ª≠ l√Ω crawl v√† l∆∞u d·ªØ li·ªáu l·∫∑p l·∫°i theo t·ª´ng round

    tgt_collection = db[TARGET_COLLECTION]

    products_list = []
    batch_start_time = time.time()

    print("\n" + "=" * 40)
    print(f"B·∫ÆT ƒê·∫¶U CRAWLING ROUND {round_number}")
    print(f"{'Chunk':<6} | {'Pending':<8} | {'OK':<6} | {'Valid':<6} | {'404':<5} | {'403':<5} | {'429':<5} | {'5xx':<5} | {'Net':<5} | {'Other':<5} | {'Time (s)':<10}")
    print("-" * 85)
    print(f"{0:<6} | {stats['total']:<8} | {stats['success']:<6} | {stats['dq_valid']:<6} | {stats['err_404']:<5} | {stats['err_403']:<5} | {stats['err_429']:<5} | {stats['err_5xx']:<5} | {stats['err_network']:<5} | {stats['err_other']:<5} | 0s")

    with open(OUTPUT_CSV, mode='a', encoding='utf-8-sig', newline='') as f:
        field_names = ['product_id', 'product_name', 'url', 'status', 'http_code', 'error_msg']

        writer = csv.DictWriter(f, fieldnames=field_names)

        # N·∫øu file ch∆∞a t·ªìn t·∫°i th√¨ ghi Header
        if not os.path.isfile(OUTPUT_CSV):
            # Ghi header
            writer.writeheader()

        # S·ª≠ d·ª•ng concurrent.futures ƒë·ªÉ c√≥ nhi·ªÅu lu·ªìng thu th·∫≠p crawl d·ªØ li·ªáu h∆°n
        # Kh·ªüi t·∫°o b·ªÉ ch·ª©a c√°c lu·ªìng
        with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:
            for item in get_product(existing_products_set):
                products_list.append(item)
                # print(len(products_list))

                if len(products_list) >= BATCH_SIZE:
                    results = list(executor.map(name_scrapping, products_list))

                    # G·ªçi h√†m x·ª≠ l√Ω trung t√¢m
                    insert_mongo_batch = process_results(results, writer, stats)

                    # Insert v√†o Mongo (Ch·ªâ data s·∫°ch)
                    if insert_mongo_batch:
                        try:
                            tgt_collection.insert_many(insert_mongo_batch, ordered=False)
                        except Exception as e:
                            print(f"ƒê√¢y l√† l·ªói khi insert v√†o MONGO: {e}")

                    # X·ª≠ l√Ω LOGGING
                    current_time = time.time()
                    batch_duration = current_time - batch_start_time
                    batch_start_time = current_time

                    chunk_num = stats['processed'] // BATCH_SIZE
                    pending = stats['total'] - stats['processed']

                    print(
                        f"{chunk_num:<6} | {pending:<8} | {stats['success']:<6} | {stats['dq_valid']:<6} | {stats['err_404']:<5} | {stats['err_403']:<5} | {stats['err_429']:<5} | "
                        f"{stats['err_5xx']:<5} | {stats['err_network']:<5} | {stats['err_other']:<5} | {batch_duration:.2f}s")

                    products_list = []  # Reset batch ƒë·∫ßu v√†o

            # Ghi n·ªët s·ªë d·ªØ li·ªáu c√≤n d∆∞
            if products_list:
                results = list(executor.map(name_scrapping, products_list))

                insert_mongo_batch = []

                # G·ªçi h√†m x·ª≠ l√Ω trung t√¢m
                insert_mongo_batch = process_results(results, writer, stats)

                # Insert v√†o Mongo (Ch·ªâ data s·∫°ch)
                if insert_mongo_batch:
                    try:
                        tgt_collection.insert_many(insert_mongo_batch, ordered=False)
                    except Exception as e:
                        print(f"ƒê√¢y l√† l·ªói khi insert v√†o MONGO: {e}")

                # X·ª≠ l√Ω LOGGING l√¥ cu·ªëi
                current_time = time.time()
                batch_duration = current_time - batch_start_time
                chunk_num = (stats['processed'] // BATCH_SIZE) + 1
                pending = 0

                print(
                    f"{chunk_num:<6} | {pending:<8} | {stats['success']:<6} | {stats['dq_valid']:<6} | {stats['err_404']:<5} | {stats['err_403']:<5} | {stats['err_429']:<5} | {stats['err_5xx']:<5} | "
                    f"{stats['err_network']:<5} | {stats['err_other']:<5} | {batch_duration:.2f}s")

    total_duration = time.time() - start_time

    # --- FINAL REPORT ---
    print("\n" + "=" * 40)
    print(f"HO√ÄN T·∫§T CRAWLING ROUND {round_number}!")
    print(f"B√ÅO C√ÅO T·ªîNG K·∫æT")
    print("-" * 50)
    print(f"T·ªïng th·ªùi gian  : {total_duration:.2f}s ({total_duration / 60:.2f} ph√∫t)")
    print("-" * 30)
    print(f"SUCCESS (200)   : {stats['success']} ({stats['success'] / stats['total'] * 100:.1f}%)")
    print(f"Data Valid      : {stats['dq_valid']}")
    print(f"Data Invalid")
    print(f" - Missing ID   : {stats['dq_missing_id']}")
    print(f" - Missing Name : {stats['dq_missing_name']}")
    print(f" - Missing Url  : {stats['dq_missing_url']}")
    print("-" * 30)
    print(f"404 Not Found   : {stats['err_404']}")
    print(f"403 Forbidden   : {stats['err_403']}")
    print(f"429 RateLimit   : {stats['err_429']}")
    print(f"5xx Server Err  : {stats['err_5xx']}")
    print(f"Network Err     : {stats['err_network']}")
    print(f"Other Errors    : {stats['err_other']}")
    print("-" * 50)
    print(f"MongoDB: ƒê√£ l∆∞u v√†o collection: '{TARGET_COLLECTION}'")
    print(f"CSV: ƒê√£ l∆∞u v√†o '{OUTPUT_CSV}'")

if __name__ == "__main__":
    db = get_database()

    start_time = time.time()

    src_collection = db[SOURCE_COLLECTION]

    print("--> ƒêang ki·ªÉm tra v√† t·∫°o Index t·ªëi ∆∞u (Cn·∫øu ch∆∞a t·ªìn t·∫°i)...")
    # T·∫°o Compound Index: Gi√∫p MongoDB l·∫•y d·ªØ li·ªáu tr·ª±c ti·∫øp t·ª´ Index (RAM)
    src_collection.create_index([
        ("collection", 1),
        ("product_id", 1),
        ("viewing_product_id", 1)
    ])

    # --- BI·∫æN TH·ªêNG K√ä CHI TI·∫æT ---
    stats = {
        'total': 0,  # T·ªïng s·ªë b·∫£n ghi
        'processed': 0,  # S·ªë b·∫£n ghi ƒë√£ x·ª≠ l√Ω
        'success': 0,  # S·ªë b·∫£n ghi th√†nh c√¥ng
        'err_404': 0,  # L·ªói 404
        'err_403': 0,  # L·ªói 403
        'err_429': 0,  # L·ªói 429
        'err_5xx': 0,  # L·ªói 5xx
        'err_network': 0,  # L·ªói m·∫•t m·∫°ng
        'err_other': 0,  # L·ªói kh√°c (Timeout, DNS...),
        # Data quality checks
        'dq_valid': 0,
        'dq_missing_id': 0,
        'dq_missing_name': 0,
        'dq_missing_url': 0
    }

    # V√íNG L·∫∂P TO√ÄN C·ª§C (GLOBAL RETRY)
    for i in range(1, MAX_RETRY_ROUNDS + 1):
        # ƒê·ªçc t·∫•t c·∫£ ID ƒë√£ l√†m xong t·ª´ tr∆∞·ªõc
        existing_products_set = load_processed_ids()

        stats['total'] = get_total() - len(existing_products_set)

        run_crawler_round(i, stats, existing_products_set, start_time)

        print(f"\n--> K·∫øt th√∫c V√≤ng {i}.")
        if i < MAX_RETRY_ROUNDS:
            print("--> Ngh·ªâ 10s tr∆∞·ªõc khi qu√©t l·∫°i c√°c m·ª•c b·ªã s√≥t/l·ªói...\n")
            time.sleep(10)
        else:
            print("--> ƒê√£ h·∫øt s·ªë l·∫ßn th·ª≠ l·∫°i (Max Retries). D·ª´ng ch∆∞∆°ng tr√¨nh.\n")

    close_connection()
    print(f"T·ªïng th·ªùi gian: {time.time() - start_time:.2f}s ({(time.time() - start_time) / 60:.2f} ph√∫t)")